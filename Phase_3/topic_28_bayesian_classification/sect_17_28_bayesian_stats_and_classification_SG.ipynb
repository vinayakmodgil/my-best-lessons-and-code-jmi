{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect 17 & 28: Bayesian Statistics & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online-ds-ft-070620\n",
    "- 09/28/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Sect 17 & Sect 28 were originally one combined section in Mod 2, which is why we waited to cover sect 17 until now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections/Topics Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Part 1, Section 17: Bayesian Statistics**\n",
    "    - Activity: Bayes Theorem Lab\n",
    "    - Activity: Maximum Likelihood Estimation - Example\n",
    "    \n",
    "    \n",
    "- **Part 2, Sect 28: Bayesian Classification**\n",
    "    - Activity: Document Classification Lab\n",
    "    - Follow-Up Activity: Document Classification Lab with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From Gdoc:\n",
    "    - Document Classification with Naive Bayes Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Part 1` Mod 2 - Sect 17: Bayesian Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Review the concept of conditional probability \n",
    "- Learn about Bayes' Theorem\n",
    "- Apply Bayes Theorem - Bayes' Theorem Lab\n",
    "- Discuss maximum likelihood estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T21:12:25.012751Z",
     "start_time": "2020-02-25T21:12:25.008886Z"
    }
   },
   "source": [
    "- **Videos**\n",
    "    - [Bayesian Stats & MLE YouTube Playlist](https://www.youtube.com/playlist?list=PLFknVelSJiSxKhi_xJIbBUZdIn49hDajE)\n",
    "\n",
    "\n",
    "- **Blog Posts & Articles**\n",
    "    - https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1\n",
    "    - [Star Wars Intro To Bayesian Priors](https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability - Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Conditional probability emerges when the outcome a trial may influence the results of the upcoming trials.**\n",
    "\n",
    "The conditional probability (Probability of $A$ **given** $B$) can be written as:\n",
    "$$ P (A \\mid B) = \\dfrac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "\n",
    "\n",
    "$P(A|B)$, is the probability A **given** that $B$ has just happened. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laws & Theorems Based on Conditional Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 1: Product Rule\n",
    "\n",
    "The intersection of events $A$ and $B$ can be given by\n",
    "\n",
    "\\begin{align}\n",
    "    P(A \\cap B) = P(B) P(A \\mid B) = P(A) P(B \\mid A)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 2: Chain Rule AKA \"General Product Rule\"\n",
    "\n",
    "- Allows calculation of any member of the join distribution of a set of random variables using _only_ conditional probabilities.\n",
    "\n",
    "- Built on the product rule: \n",
    "$$P(A \\cap B) = P(A \\mid B) P(B)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem - Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Starts with the formula for conditional probability/likelihood:\n",
    "\n",
    "$$ P(A|B) = \\dfrac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "- Substitute $P(B|A)P(A)$ for $P(A \\cap B)$ using the product rule and we get:\n",
    "\n",
    "\n",
    "\n",
    "#### Bayes' Theorem\n",
    "\n",
    "$$ \\large P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "\n",
    "- Note that, using Bayes theorem, you can compute conditional probabilities without explicitly needing to know $P(A \\cap B)$! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⏰ Activity: Bayes' Theorem - Lab (Mod 2 - Sect 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom function for Bayes' theorem\n",
    "\n",
    "To start, write a function, `bayes()`, which takes in the probability of A, the probability of B, and the probability of B given A. From this, the function should then return the conditional probability of A, given that B is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:21:34.351935Z",
     "start_time": "2020-09-28T00:21:34.349426Z"
    }
   },
   "outputs": [],
   "source": [
    "def bayes(P_a, P_b, P_b_given_a):\n",
    "    # Your code here\n",
    "    return P_a_given_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skin Cancer\n",
    "\n",
    "After a physical exam, a doctor observes a blemish on a client's arm. The doctor is concerned that the blemish could be cancerous, but tells the patient to be calm and that it's probably benign. Of those with skin cancer, 100% have such blemishes. However, 20% of those without skin cancer also have such blemishes. If 15% of the population has skin cancer, **what's the probability that this patient has skin cancer?**\n",
    "\n",
    "> Hint: Be sure to calculate the overall rate of blemishes across the entire population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Must apply the Law of Total Probability to get P_blemish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:42:12.797271Z",
     "start_time": "2020-09-27T22:42:12.793176Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:42:12.802010Z",
     "start_time": "2020-09-27T22:42:12.799300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:21:34.360932Z",
     "start_time": "2020-09-28T00:21:34.358604Z"
    }
   },
   "outputs": [],
   "source": [
    "P_cancer_given_blemish = None\n",
    "P_cancer_given_blemish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0.46875`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children (I) \n",
    " \n",
    "A couple has two children, the older of which is a boy. What is the probability that they have two boys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:42:12.810979Z",
     "start_time": "2020-09-27T22:42:12.808586Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:21:34.367187Z",
     "start_time": "2020-09-28T00:21:34.364957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your solution P(2boys|older child is a boy)\n",
    "P_2boys_given_oldboy = None\n",
    "P_2boys_given_oldboy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children  (II)\n",
    "\n",
    "A couple has two children, one of which is a boy. What is the probability that they have two boys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:42:12.823140Z",
     "start_time": "2020-09-27T22:42:12.820671Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:21:34.374369Z",
     "start_time": "2020-09-28T00:21:34.372147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your solution P(2boys|1 of 2 children is a boy)\n",
    "P_2boys_given_1boy = None\n",
    "P_2boys_given_1boy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0.3333333333333333`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A diagnostic test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diagnostic test is advertised as being 99% accurate \n",
    "\n",
    "* If a patient has the disease, they  will test positive 99% of the time \n",
    "\n",
    "* If they don't have the disease, they will test negative 99% of the time  \n",
    "\n",
    "* 1% of all people have this disease \n",
    "\n",
    "If a patient tests positive, what is the probability that they actually have the disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:21:34.381394Z",
     "start_time": "2020-09-28T00:21:34.379257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your solution P(Disease | positive test)\n",
    "P_disease_given_pos = None\n",
    "P_disease_given_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MLE primarily deals with **determining the parameters ($\\theta$'s)** that **maximize the probability/liklihood of observing the data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T00:17:06.189085Z",
     "start_time": "2020-02-23T00:17:06.186800Z"
    }
   },
   "source": [
    "### Parameter Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we have observations for a phenomenon that we do not know the probability/parameters for, we can use the probability of seeing those observations (the likelihood) for different possible parameters until we find the value for the parameter that **maximizes our chances/likelihood of seeing the observed data.**'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations are independent \n",
    "- Observations are identically distributed\n",
    "\n",
    "\n",
    "> These assumptions are so common they have been given an abbreviation: \"the i.i.d. assumption (independent and identically distributed samples)\n",
    "\n",
    "<!---<img src =\"https://raw.githubusercontent.com/learn-co-students/dsc-mle-online-ds-pt-100719/master/images/der.png\">--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T21:18:55.524353Z",
     "start_time": "2020-02-25T21:18:55.522341Z"
    }
   },
   "source": [
    "## ⏰ Activity: Using MLE to find the Mean and Std for Male Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:36:36.589600Z",
     "start_time": "2020-09-28T16:36:36.587329Z"
    }
   },
   "outputs": [],
   "source": [
    "from mle import *#plot_sample_dist, plot_sample_rug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:36:37.790395Z",
     "start_time": "2020-09-28T16:36:37.561477Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U fsds\n",
    "from fsds.imports import *\n",
    "df= fs.datasets.load_height_weight()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:36:38.089448Z",
     "start_time": "2020-09-28T16:36:38.081497Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get sample of males\n",
    "df_male =df.groupby('Gender').get_group('Male')['Height']\n",
    "male_sample = df_male.sample(100,random_state=123)\n",
    "male_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:36:38.698340Z",
     "start_time": "2020-09-28T16:36:38.583862Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_sample_rug(male_sample);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **- Based on the plot above, what would you guess is the mean and standard deviation of the male population?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Parameter Inference to Estimate Population Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:36:39.956746Z",
     "start_time": "2020-09-28T16:36:39.779361Z"
    }
   },
   "outputs": [],
   "source": [
    "## Let's guess a mean of 62, and std=2 to start\n",
    "guess_mean = 62\n",
    "guess_std = 2\n",
    "\n",
    "plot_sample_dist(male_sample,guess_mean,guess_std);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Calculate the Likelihood of these samples given the mean and std we guessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Density Function of a Normal Distribution\n",
    "\n",
    "https://towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability density function equation for the normal distribution is given by the following expression:\n",
    "\n",
    "$$ P(x) = \\dfrac{1}{\\sigma \\sqrt {2\\pi }}e^{-\\dfrac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "Here, \n",
    "- $\\mu$ is the mean\n",
    "- $\\sigma$ is the standard deviation\n",
    "- $\\pi \\approx 3.14159 $ \n",
    "- $ e \\approx 2.71828 $\n",
    "\n",
    "\n",
    "> - or we could just use `scipy.stats.norm.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:36:41.846148Z",
     "start_time": "2020-09-28T16:36:41.841185Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate the likelihood of our sample using our guess\n",
    "likelihood = stats.norm.pdf(male_sample,loc=guess_mean,\n",
    "                            scale=guess_std)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Total Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:07.917205Z",
     "start_time": "2020-09-28T16:37:07.913752Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calc total likelihood\n",
    "likelihood.prod() #Ruh Roh..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding \"underflow\"\n",
    "\n",
    "> \"...repeatedly multiplying small probabilities can lead to underflow; rounding to zero due to numerical approximation limitations. As such, **a common alternative is to add the logarithms of the probabilities as opposed to multiplying the raw probabilities** themselves...\"<br>\n",
    "$$ \\large e^x \\cdot e^y = e^{x+y}$$  \n",
    "$$ \\large log_{e}(e)=1 $$  \n",
    "$$\\large  e^{log(x)} = x$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:13.140903Z",
     "start_time": "2020-09-28T16:37:13.137489Z"
    }
   },
   "outputs": [],
   "source": [
    "## Log likelihood\n",
    "np.log(likelihood).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Activity: write a function to calculate total likelihood:\n",
    "- It should accept:\n",
    "    - the sample\n",
    "    - the value for the mean\n",
    "    - the value for the std\n",
    "- It should calculate the total likelihood of observing all of the samples given the mean and std.\n",
    "- Finally, It should flexibly return either log-likehood or raw likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:23:36.697598Z",
     "start_time": "2020-09-28T00:23:36.695086Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_total_likelihood():\n",
    "    \"\"\"Calculates prob of sample given the mean and std\"\"\"\n",
    "    # Calc prob of each point\n",
    "    \n",
    "    ## If want log-likelihood, sum the logged probs\n",
    "    \n",
    "    ## otherwise multiply the probabilities together\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:39:46.683620Z",
     "start_time": "2020-09-28T16:39:46.671801Z"
    }
   },
   "outputs": [],
   "source": [
    "## Test out the function with our guess\n",
    "guess_mean = 62\n",
    "guess_std = 2\n",
    "\n",
    "calc_total_likelihood(male_sample,guess_mean,guess_std,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:39:32.553051Z",
     "start_time": "2020-09-28T16:39:32.364038Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plot sample vs pdf\n",
    "plot_sample_dist(male_sample,guess_mean,guess_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Let's test many values for mean and std to find the most likely values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Inference\n",
    "- We want to infer which of these values best matches the true Mean and Std of male height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:23:46.953287Z",
     "start_time": "2020-09-28T00:23:46.949401Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_mus = np.arange(42,80,0.1)\n",
    "theta_mus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:24:03.601948Z",
     "start_time": "2020-09-28T00:24:03.597421Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_stds = np.arange(0.2,5,0.2)\n",
    "theta_stds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:24:44.618781Z",
     "start_time": "2020-09-28T00:24:44.612889Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "theta_params = list(itertools.product(theta_mus,theta_stds))\n",
    "display(theta_params[:5])\n",
    "print(f\"There are {len(theta_params)} combinations of params to try.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:25:06.593834Z",
     "start_time": "2020-09-28T00:25:06.591461Z"
    }
   },
   "outputs": [],
   "source": [
    "## In a Loop, try out all combinations of mu and std and save results\n",
    "results = [['Mu','Std','Likelihood']]\n",
    "\n",
    "for something in []:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:25:14.589430Z",
     "start_time": "2020-09-28T00:25:14.587356Z"
    }
   },
   "outputs": [],
   "source": [
    "## Turn Results into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:53.814461Z",
     "start_time": "2020-09-28T00:12:53.805302Z"
    }
   },
   "outputs": [],
   "source": [
    "## Sort the results to find the max likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:25:52.112974Z",
     "start_time": "2020-09-28T00:25:52.110820Z"
    }
   },
   "outputs": [],
   "source": [
    "## Pull out best params\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.014378Z",
     "start_time": "2020-09-28T00:12:53.822277Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plot the sample and best params using plot_sample_dist\n",
    "plot_sample_dist(male_sample,best_params['Mu'],best_params['Std']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.019987Z",
     "start_time": "2020-09-28T00:12:54.016146Z"
    }
   },
   "outputs": [],
   "source": [
    "## How do they compare to sample mean and std?\n",
    "male_sample.mean(),male_sample.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.026012Z",
     "start_time": "2020-09-28T00:12:54.021791Z"
    }
   },
   "outputs": [],
   "source": [
    "## How do they compare to population mean and std?\n",
    "df_male.mean(),df_male.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### We just inferred the mean and std parameters using Maximum Likelihood Estimation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Part 2` Mod 3 - Sect 28: Bayesian Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand how Bayes theorem can be applied to classify data using conditional probabilities.\n",
    "\n",
    "- Understand Gaussian Naive Bayes and how it uses the Probability Density Function of a Normal Distribution \n",
    "\n",
    "- Understand the \"underflow\" issue and how to fix.\n",
    "\n",
    "\n",
    "- Apply Naive Bayes manually and with sklearn\n",
    "\n",
    "    - Activity 1: Gaussian Naive Bayes Lab\n",
    "    - Activity 2: Document Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large P(A|B) = \\dfrac{P(B|A)(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The Bayesian interpretation of this formula is***\n",
    "\n",
    "\n",
    "\n",
    "$$ \\large P(A|B) = \\dfrac{P(B|A)(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "$$ \\large \\text{Posterior} = \\dfrac{\\text{Likelihood} \\cdot \\text{Prior}}{\\text{Evidence}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes makes the assumption that our probabilities follow a normal distribution.\n",
    "- It uses the Probability Density Function for a Normal (Gaussian) Distribution to get point estimates of the probabilities.\n",
    "\n",
    "> **Note: above we used the normal distribution probability density function to estimate likelihood, which is essentiall what Guassian Naive Bayes does!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.102963Z",
     "start_time": "2020-09-28T00:12:54.035602Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = iris.feature_names\n",
    "\n",
    "y = pd.DataFrame(iris.target)\n",
    "y.columns = ['Target']\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.142754Z",
     "start_time": "2020-09-28T00:12:54.106533Z"
    }
   },
   "outputs": [],
   "source": [
    "aggs = df.groupby('Target').agg(['mean', 'std'])\n",
    "aggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.154695Z",
     "start_time": "2020-09-28T00:12:54.144974Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_x_given_class(obs_row, feature, class_):\n",
    "    mu = aggs[feature]['mean'][class_]\n",
    "    std = aggs[feature]['std'][class_]\n",
    "    \n",
    "    # A single observation\n",
    "    obs = df.iloc[obs_row][feature] \n",
    "    \n",
    "    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n",
    "    return p_x_given_y\n",
    "\n",
    "# Notice how this is not a true probability; you can get values > 1\n",
    "p_x_given_class(0, 'petal length (cm)', 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.177206Z",
     "start_time": "2020-09-28T00:12:54.157099Z"
    }
   },
   "outputs": [],
   "source": [
    "row = 100\n",
    "c_probs = []\n",
    "for c in range(3):\n",
    "    # Initialize probability to relative probability of class \n",
    "    p = len(df[df['Target'] == c])/len(df) \n",
    "    for feature in X.columns:\n",
    "        p *= p_x_given_class(row, feature, c) \n",
    "        # Update the probability using the point estimate for each feature\n",
    "        c_probs.append(p)\n",
    "\n",
    "c_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.182492Z",
     "start_time": "2020-09-28T00:12:54.178916Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(row):\n",
    "    c_probs = []\n",
    "    for c in range(3):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(df[df['Target'] == c])/len(df) \n",
    "        for feature in X.columns:\n",
    "            p *= p_x_given_class(row, feature, c)\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:54.200968Z",
     "start_time": "2020-09-28T00:12:54.184031Z"
    }
   },
   "outputs": [],
   "source": [
    "row = 0\n",
    "df.iloc[row]\n",
    "predict_class(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:56.122440Z",
     "start_time": "2020-09-28T00:12:54.202616Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Predictions'] =  [predict_class(row) for row in df.index]\n",
    "df['Correct?'] = df['Target'] == df['Predictions']\n",
    "df['Correct?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding \"underflow\"\n",
    "\n",
    "> \"...repeatedly multiplying small probabilities can lead to underflow; rounding to zero due to numerical approximation limitations. As such, a common alternative is to add the logarithms of the probabilities as opposed to multiplying the raw probabilities themselves...\"<br>\n",
    "$$ \\large e^x \\cdot e^y = e^{x+y}$$  \n",
    "$$ \\large log_{e}(e)=1 $$  \n",
    "$$\\large  e^{log(x)} = x$$ \n",
    "\n",
    "With that, here's an updated version of the function using log probabilities to avoid underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:56.127514Z",
     "start_time": "2020-09-28T00:12:56.124029Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class_log(row):\n",
    "    c_probs = []\n",
    "    for c in range(3):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(df[df['Target'] == c])/len(df) \n",
    "        for feature in X.columns:\n",
    "            p += np.log(p_x_given_class(row, feature, c))\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:12:58.152387Z",
     "start_time": "2020-09-28T00:12:56.129103Z"
    }
   },
   "outputs": [],
   "source": [
    "row = 0\n",
    "\n",
    "df.iloc[row]\n",
    "print(predict_class_log(row))\n",
    "df['Predictions'] =  [predict_class_log(row) for row in df.index]\n",
    "df['Correct?'] = df['Target'] == df['Predictions']\n",
    "df['Correct?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\large P(\\text{Spam | Word}) = \\dfrac{P(\\text{Word | Spam})P(\\text{Spam})}{P(\\text{Word})}$$  \n",
    "\n",
    "- Where $P(\\text{Word | Spam})$ is\n",
    "\n",
    " $$ \\large P(\\text{Word | Spam}) = \\dfrac{\\text{Word Frequency in Document}}{\\text{Word Frequency Across All Spam Documents}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"However, this formulation has a problem: **what if you encounter a word in the test set that was not present in the training set?** This new word would have a frequency of zero! To effectively counteract these issues, Laplacian smoothing is often used giving:\"  \n",
    "\n",
    "- ***Laplacian smoothing:***\n",
    "\n",
    " $$P(\\text{Word | Spam}) = \\dfrac{\\text{Word Frequency in Document} + 1}{\\text{Word Frequency Across All Spam Documents + Number of Words in Corpus Vocabulary}}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⏰ Activity:  Document Classification with Naive Bayes Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn Student Repo: https://github.com/learn-co-students/dsc-document-classification-with-naive-bayes-lab-onl01-dtsc-ft-070620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-Up Bonus Activity: Doing the lab with `sklearn` (if there's time/demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the same classification task using `sklearn` with Natural Language Processing tools\n",
    "\n",
    "```python \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=stopwords_list)\n",
    "\n",
    "## X is intact text, y is label\n",
    "X = df2['text']\n",
    "y = df2['label'].copy()\n",
    "\n",
    "X_train, X_test, y_train,y_test =train_test_split(X, y)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using  sklearn MultinomialNB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "```python\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec,y_train)\n",
    "\n",
    "y_hat_test = model.predict(X_test_vec)\n",
    "\n",
    "print(metrics.classification_report(y_test,y_hat_test))\n",
    "metrics.plot_confusion_matrix(model,X_test_vec,y_test,cmap='Blues', normalize='true')\n",
    "# y_test.value_counts(normalize=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sect 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Monotonic function\n",
    "\n",
    "> In mathematics, a [monotonic function](https://en.wikipedia.org/wiki/Monotonic_function) (or monotone function) is a function between ordered sets that preserves or reverses the given order. This concept first arose in calculus, and was later generalized to the more abstract setting of order theory. \n",
    "\n",
    "\n",
    "According to this theory, if you apply a monotonic function to another function, like the one you are trying to optimize above, this application will preserve the critical points (maxima in this case) of the original function. Logarithmic functions are normally used within the domain of machine learning to achieve the functionality of monotonicity. The logarithmic function is described as:\n",
    "\n",
    "> $log_b(x)$\n",
    "\n",
    "* where b is any number such that b > 0, b ≠ 1, and x > 0  \n",
    "* The function is read \"log base b of x\" \n",
    "\n",
    "The logarithm y is the exponent to which b must be raised to get x. The behavior of a log function can be understood from following image.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-mle-online-ds-pt-100719/master/images/new_log.png\" width=\"700\">\n",
    "\n",
    "\n",
    "This helps you realize that **log of f(θ) i.e. log(f(θ)) will have the save maxima as the likelihood function f(θ).** This is better known as the **log likelihood**. \n",
    "\n",
    "Thus, the optimization function i.e. $\\theta^6(1-\\theta)^4$ , that you're trying to optimize w.r.t. theta can be written down as:\n",
    "\n",
    "> $\\underset{\\theta}{\\operatorname{argmax}} \\theta^6(1-\\theta)^4$\n",
    "\n",
    "> In mathematics, the arguments of the maxima (abbreviated arg max or argmax) are the points of the domain of some function at which the function values are maximized. \n",
    "\n",
    "Remember that you are not concerned with the actual maximum value of the function. You want to **learn the value for theta** where the **function has the maximum value**.\n",
    "\n",
    "Following the monotonicity principle, the argmax function can be written with natural log *ln* as:\n",
    "\n",
    "> $\\underset{\\theta}{\\operatorname{argmax}} ln(\\theta^6(1-\\theta)^4)$\n",
    " \n",
    "> $=\\underset{\\theta}{\\operatorname{argmax}} 6 (ln (\\theta)) + 4 (ln(1-\\theta))$\n",
    "\n",
    "Let's call our log likelihood function $g(\\theta)$, take its derivative and set it to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "### The differences between Bayesians and Frequentists:\n",
    "- Their interpretation of probability itself. \n",
    "    - For Frequentists, the probability of an event is the limit of the rate of occurrences of the event if the same scenario including context and assumptions were repeated ad infinitum. \n",
    "    - In contrast, Bayesians interpret probability as the level of confidence, or belief, in a particular event occurring.    \n",
    "\n",
    "- The practical implications of Bayesians versus Frequentists rest upon making assumptions about unknown quantities:\n",
    "    - In the Bayesian framework, you make assumptions about unknown variables which you are attempting to estimate. For example, you might assume that the number of individuals who will buy a product can be represented by a binomial variable with parameter $p$.\n",
    "    - In contrast, the Frequentist perspective does not allow embedding of prior beliefs such as this into statistical experiments and analyses.\n",
    "\n",
    "In many ways, this makes a more natural interpretation for rare events that cannot possibly reoccur in the same context and circumstances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "442px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
