{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 39: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 12/01/20\n",
    "- online-ds-ft-081720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduce the field of Natural Language Processing\n",
    "- Learn about the extensive preprocessing involved with text data\n",
    "- Walk through text classification - Finding Trump \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Questions Doc](https://docs.google.com/document/d/1oJLFYAGc-tCDrKtlDVSxfWK4fBNCMgL69hY3-ej6QXk/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is NLP Used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Reviews (i.e. Amazon)\n",
    "- Stock market trading\n",
    "- AI Assistants\n",
    "- Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demonstrations**\n",
    "- [Google Duplex AI Assistant](https://youtu.be/D5VN56jQMWM)\n",
    "- [GPT2 Blog Post](https://openai.com/blog/better-language-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing text data requires more processing than normal data.**\n",
    "1. We must remove things like:\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters\n",
    "    \n",
    "    \n",
    "2. It is always recommended that go a step beyond this and remove **commonly used words that contain little information (called \"stopwords\")** for our machine learning algorithms. Words like: the,was,he,she, it,etc.\n",
    "\n",
    "\n",
    "3. Additionally, most analyses **need the text tokenzied** into a list of words and not in a natural sentence format. Instead, they are a list of words (**tokens**) separated by \"`,`\", which tells the algorithm what should be considered one word.\n",
    "\n",
    "\n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "There are often **multiple variants of the same word with the same/simiar meaning**,<br> but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).**<br> Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><br> A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"<br>  \n",
    "    - Stemming<br><img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-nlp-and-word-vectorization-online-ds-ft-100719/master/images/new_stemming.png\" width=40%>\n",
    "    - Lemmatization\n",
    "    \n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |\n",
    "\n",
    "5. Finally, we have to convert our text data into numeric form for our machine learning models to analyze, a process called **vectorization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Practicing Text Preprocessing - Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Natural Language Processing Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare Donal Trump's tweets for modeling, **it is essential to preprocess the text** and simplify its contents.\n",
    "<br><br>\n",
    "1. **At a minimum, things like:**\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters<br>\n",
    "    ***must*** be addressed before any initial analyses. I refer tho this initial cleaning as **\"minimal cleaning\"** of the text content<br>\n",
    "    \n",
    "> Version 1 of the tweet processing removes these items, as well as the removal of any urls in a tweet. The resulting data column is referred to here as \"content_min_clean\".\n",
    "\n",
    "<br><br>\n",
    "2. It is **always recommended** that go a step beyond this and<br> remove **commonly used words that contain little information** <br>for our machine learning algorithms. Words like: (the,was,he,she, it,etc.)<br> are called **\"stopwords\"**, and it is critical to address them as well.\n",
    "\n",
    "> Version 2 of the tweet processing removes these items and the resulting data column is referred here as `cleaned_stopped_content`\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Additionally, many analyses **need the text tokenzied** into a list of words<br> and not in a natural sentence format. Instead, they are a list of words (**tokens**) separated by \",\", which tells the algorithm what should be considered one word.<br><br>For the tweet processing, I used a version of tokenization, called `regexp_tokenziation` <br>which uses pattern of letters and symbols (the `expression`) <br>that indicate what combination of alpha numeric characters should be considered a single token.<br><br>The pattern I used was `\"([a-zA-Z]+(?:'[a-z]+)?)\"`, which allows for words such as \"can't\" that contain \"'\" in the middle of word. This processes was actually applied in order to process Version 1 and 2 of the Tweets, but the resulting text was put back into sentence form. \n",
    "\n",
    "> Version 3 of the tweets keeps the text in their regexp-tokenized form and is reffered to as `cleaned_stopped_tokens`\n",
    "<br>\n",
    "\n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "There are often **multiple variants of the same word with the same/simiar meaning**,<br> but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).**<br> Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><br> A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"<br>  \n",
    "\n",
    "> Version 4 of the tweets are all reduced down to their word lemmas, futher aiding the algorithm in learning the meaning of the texts.\n",
    "<!-- \n",
    "\n",
    "#### EXAMPLE TWEETS AND PROCESSING STEPS:\n",
    "\n",
    "**TWEET FROM 08-25-2017 12:25:10:**\n",
    "* **[\"content\"] column:**<p><blockquote>***\"Strange statement by Bob Corker considering that he is constantly asking me whether or not he should run again in '18. Tennessee not happy!\"***\n",
    "    \n",
    "    \n",
    "* **[\"content_min_clean\"] column:**<p><blockquote>***\"strange statement by bob corker considering that he is constantly asking me whether or not he should run again in  18  tennessee not happy \"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_content\"] column:**<p><blockquote>***\"strange statement bob corker considering constantly asking whether run tennessee happy\"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_tokens\"] column:**<p><blockquote>***\"['strange', 'statement', 'bob', 'corker', 'considering', 'constantly', 'asking', 'whether', 'run', 'tennessee', 'happy']\"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_lemmas\"] column:**<p><blockquote>***\"strange statement bob corker considering constantly asking whether run tennessee happy\"*** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:24.691727Z",
     "start_time": "2020-12-01T18:41:22.858098Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:24.761123Z",
     "start_time": "2020-12-01T18:41:24.693807Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "finding_trump = 'finding-trump.csv'\n",
    "df = pd.read_csv(finding_trump)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:42:46.943562Z",
     "start_time": "2020-12-01T18:42:46.941291Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create a variable \"corpus\" containing all text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Bag-of-Words Frequency Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"bag-of-words\": collection of all words from a corpus and their frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:42:57.805712Z",
     "start_time": "2020-12-01T18:42:57.078857Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "## Make a FreqDist from the corpus\n",
    "\n",
    "## Display 100 most common words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> That's not quite right..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:10.519493Z",
     "start_time": "2020-12-01T18:43:10.517213Z"
    }
   },
   "outputs": [],
   "source": [
    "## Tokenize corpus then generate FreqDist\n",
    "from nltk import word_tokenize\n",
    "\n",
    "## Convert Corpus to Tokens\n",
    "\n",
    "\n",
    "## Get FreqDist and most_common 100 for tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Better...but what's our next issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:17.889855Z",
     "start_time": "2020-12-01T18:43:17.887796Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:32.391278Z",
     "start_time": "2020-12-01T18:43:32.389214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language and preview first 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:42.103068Z",
     "start_time": "2020-12-01T18:43:42.101074Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add punctuation to stopwords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:45.979682Z",
     "start_time": "2020-12-01T18:43:45.977304Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add the additional Tweet Punctuation below to stopwords_list\n",
    "additional_punc = ['“','”','...',\"''\",'’','``']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:49.836391Z",
     "start_time": "2020-12-01T18:43:49.834322Z"
    }
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:53.998462Z",
     "start_time": "2020-12-01T18:43:53.996363Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove until from stopwords_list and check for it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:43:57.870619Z",
     "start_time": "2020-12-01T18:43:57.868441Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:44:01.915099Z",
     "start_time": "2020-12-01T18:44:01.912966Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remake the FreqDist from stopped_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ways to Show Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Word Clouds](https://www.geeksforgeeks.org/generating-word-cloud-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:45:35.549953Z",
     "start_time": "2020-12-01T18:45:35.493213Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "## Initalize a WordCloud with our stopwords_list and no bigrams\n",
    "\n",
    "## Generate wordcloud from stopped_tokens\n",
    "\n",
    "## Plot with matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:45:40.263347Z",
     "start_time": "2020-12-01T18:45:40.043597Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopwords_list += additional_punc\n",
    "    stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What recognizable pattern of characters is high on the frequency list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Bag of Words Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:32.318748Z",
     "start_time": "2020-12-01T18:41:31.238156Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweet_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:32.415751Z",
     "start_time": "2020-12-01T18:41:32.320965Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a DataFrame from the Bigrams\n",
    "pd.DataFrame(tweets_scored, columns=[\"Word\",\"Freq\"]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.009728Z",
     "start_time": "2020-12-01T18:41:32.417635Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "tweet_pmi_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweet_pmi_finder.apply_freq_filter(5)\n",
    "\n",
    "tweet_pmi_scored = tweet_pmi_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:45:56.675225Z",
     "start_time": "2020-12-01T18:45:56.673133Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a DataFrame from the Bigrams with PMI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regular expressions can help us capture/remove complicated patterns in our text.\n",
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side.\n",
    "    \n",
    "    \n",
    "- Let's use regular expressions to remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.027689Z",
     "start_time": "2020-12-01T18:41:33.024214Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select an example tweet\n",
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.037903Z",
     "start_time": "2020-12-01T18:41:33.034497Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select a second example tweet\n",
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.046696Z",
     "start_time": "2020-12-01T18:41:33.042359Z"
    }
   },
   "outputs": [],
   "source": [
    "## From the lessons\n",
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use regex to find/remove URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- www.regex101.com\n",
    "    - Copy and paste example text to search\n",
    "    - Test out regular expressions and see what they pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.051483Z",
     "start_time": "2020-12-01T18:41:33.048736Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.057297Z",
     "start_time": "2020-12-01T18:41:33.053335Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r\"(https://\\w*\\.\\w*/+\\w+)\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.063523Z",
     "start_time": "2020-12-01T18:41:33.059219Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    \n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.069205Z",
     "start_time": "2020-12-01T18:41:33.065139Z"
    }
   },
   "outputs": [],
   "source": [
    "## Other uses of RegEx for Tweet preprocessing\n",
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.074267Z",
     "start_time": "2020-12-01T18:41:33.071217Z"
    }
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.078775Z",
     "start_time": "2020-12-01T18:41:33.075633Z"
    }
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:33.083618Z",
     "start_time": "2020-12-01T18:41:33.080379Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:34.779038Z",
     "start_time": "2020-12-01T18:41:33.085292Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:34.785044Z",
     "start_time": "2020-12-01T18:41:34.781202Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "text_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:46:26.663923Z",
     "start_time": "2020-12-01T18:46:26.661397Z"
    }
   },
   "outputs": [],
   "source": [
    "# def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "# #     text=text.copy()\n",
    "#     for x in find_urls(text):\n",
    "#         text = text.replace(x,'')\n",
    "        \n",
    "#     for x in find_retweets(text):\n",
    "#         text = text.replace(x,'')    \n",
    "        \n",
    "#     for x in find_hashtags(text):\n",
    "#         text = text.replace(x,'')    \n",
    "\n",
    "#     if as_lemmas:\n",
    "#         from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "#     if as_tokens:\n",
    "#         text = clean_text(text)\n",
    "    \n",
    "#     if len(text)==0:\n",
    "#         text=''\n",
    "            \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:46:28.376767Z",
     "start_time": "2020-12-01T18:46:28.374791Z"
    }
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def show_processed_text(i=(0,len(corpus)-1)):\n",
    "#     text_in = corpus[i]#.copy()\n",
    "#     print(text_in)\n",
    "#     text_out = process_tweet(text_in)\n",
    "#     print(text_out)\n",
    "#     text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "#     print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:46:31.138466Z",
     "start_time": "2020-12-01T18:46:31.136432Z"
    }
   },
   "outputs": [],
   "source": [
    "# corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For computers to process text it needs to be converted to a numerical representation of the text.\n",
    "- **There are several different ways we can vectorize our text:**\n",
    "    - Count vectorization\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "        -  Used for multiple texts\n",
    "    - Word Embeddings (Deep NLP)\n",
    "    \n",
    "    \n",
    ">- **_Term Frequency_** is calculated with the following formula:\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ <br>\n",
    "- Which can also be represented as:\n",
    "$$\\begin{align}\n",
    " \\text{tf}_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $$\n",
    "\n",
    "> - **_Inverse Document Frequency_** is calculated with the following formula:\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$<br>\n",
    "- Which can also be represented as: \n",
    "$$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $$\n",
    "\n",
    "> The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n",
    "$$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing } i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $$\n",
    "\n",
    "- There are additional ways to vectorize using Deep Neural Networks to create Word Embeddings (see Module 4 > Appendix: Deep NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Do we remove stop words or not?    \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?   \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?   \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVITY: Text Classifcation - Finding Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task - Finding Trump\n",
    "\n",
    "> - All presidents have staffers help maintain their social media presence on their behalf. \n",
    "- Early During His Presidency, Donald Trump refused to stop using his insecure and unofficial Android Phone\n",
    "- During this time period, his staffers were the ones Tweeting from the official presidential iPhone.\n",
    "\n",
    "> - Therefore, if we isolate our dataset to ONLY the times where Trump's account was posting from BOTH android and iphone, we can then assume that Android Tweets are Trump and that iPhone tweets are his staffers.\n",
    "\n",
    "> #### Now that we know that... let's build a NLP classification model to Find Trump!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:47:42.365238Z",
     "start_time": "2020-12-01T18:47:42.126712Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "finding_trump = 'finding-trump.csv'\n",
    "\n",
    "## Load in the df with created_at as dt index\n",
    "\n",
    "\n",
    "## preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:47:44.497793Z",
     "start_time": "2020-12-01T18:47:44.495720Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check Value Counts for Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:47:48.391033Z",
     "start_time": "2020-12-01T18:47:48.388919Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get time period where Trump still had his personal Android\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:47:52.288108Z",
     "start_time": "2020-12-01T18:47:52.285857Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get Start_ts and end_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:47:58.717118Z",
     "start_time": "2020-12-01T18:47:58.715104Z"
    }
   },
   "outputs": [],
   "source": [
    "## Slice out the data from start_ts to end_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:00.738427Z",
     "start_time": "2020-12-01T18:48:00.736463Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check new value counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:03.484088Z",
     "start_time": "2020-12-01T18:48:03.482013Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove the Web tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:06.636749Z",
     "start_time": "2020-12-01T18:48:06.634264Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make new Trump Tweet Column of 0 and 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:11.724747Z",
     "start_time": "2020-12-01T18:48:11.722665Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:16.385539Z",
     "start_time": "2020-12-01T18:48:16.383265Z"
    }
   },
   "outputs": [],
   "source": [
    "## Train Test Split (random state=42)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:18.777522Z",
     "start_time": "2020-12-01T18:48:18.775238Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check y_train and y_test value counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:24.271087Z",
     "start_time": "2020-12-01T18:48:24.268627Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "## Make a TweekTokenizer from nltk.tokenize (preserve_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:36.237643Z",
     "start_time": "2020-12-01T18:41:36.136293Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "## Make a TfIdf Vectorizer using tweet tokenizer's .tokenize method\n",
    "\n",
    "\n",
    "# Vectorize data and make X_train_tfidf and X_test_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:48:45.697445Z",
     "start_time": "2020-12-01T18:48:45.695309Z"
    }
   },
   "outputs": [],
   "source": [
    "## check lenght ofn the vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:36.524713Z",
     "start_time": "2020-12-01T18:41:36.248050Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Make and fit a random forest  (class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:36.547557Z",
     "start_time": "2020-12-01T18:41:36.526760Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get predictions for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:36.554480Z",
     "start_time": "2020-12-01T18:41:36.549262Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(y_test,y_hat_test,X_test,clf=None,\n",
    "                  scoring=metrics.recall_score,verbose=False,\n",
    "                  scorer=False,classes=['Not Trump','Trump']):\n",
    "    \"\"\"Quick/simple classification model evaluatiin\"\"\"\n",
    "\n",
    "    print(metrics.classification_report(y_test,y_hat_test,\n",
    "                                        target_names=classes))\n",
    "    \n",
    "    metrics.plot_confusion_matrix(clf,X_test,y_test,normalize='true',\n",
    "                                 cmap='Blues',display_labels=classes)\n",
    "    plt.show()\n",
    "    if verbose:\n",
    "        print(\"MODEL PARAMETERS:\")\n",
    "        print(pd.Series(rf.get_params()))\n",
    "        \n",
    "    if scorer:\n",
    "        \n",
    "        return scoring(y_test,y_hat_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:49:04.745728Z",
     "start_time": "2020-12-01T18:49:04.743521Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate Model using function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:37.132620Z",
     "start_time": "2020-12-01T18:41:36.778801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the top 30 most important features\n",
    "with plt.style.context('seaborn-talk'):\n",
    "\n",
    "    ## Get Feature Importance\n",
    "\n",
    "    ## Take the .tail 30 and plot kind='barh'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note About Pipelines and GridSearch for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may want to to this process in multiple steps (first Count Vectorize, then transform to TF or TF-IDF.\n",
    "- Can then use these in a Pipeline to be able to GridSearch more aspect of the text preprocessing\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "#tf_transformer.fit(X_train_counts)\n",
    "#X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "#X_train_tf.shape\n",
    "\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:37.159023Z",
     "start_time": "2020-12-01T18:41:37.134588Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tf_transform = TfidfTransformer(use_idf=True)\n",
    "\n",
    "text_pipe = Pipeline(steps=[\n",
    "    ('count_vectorizer',count_vect),\n",
    "    ('tf_transformer',tf_transform)])\n",
    "\n",
    "full_pipe = Pipeline(steps=[\n",
    "    ('text_pipe',text_pipe),\n",
    "    ('clf',RandomForestClassifier(class_weight='balanced'))\n",
    "])\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:37.185287Z",
     "start_time": "2020-12-01T18:41:37.160812Z"
    }
   },
   "outputs": [],
   "source": [
    "## Preview current X_train\n",
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:37.188985Z",
     "start_time": "2020-12-01T18:41:37.186880Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make X_train_pipe and X_test_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:37.193176Z",
     "start_time": "2020-12-01T18:41:37.190985Z"
    }
   },
   "outputs": [],
   "source": [
    "## Modeling with full pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:37.214831Z",
     "start_time": "2020-12-01T18:41:37.195255Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='text')\n",
    "\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearching NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.402970Z",
     "start_time": "2020-12-01T18:41:37.216681Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "## Make a tokenizer with TweetTokenizer\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False,)\n",
    "vectorizer = CountVectorizer()\n",
    "## Make params Grid\n",
    "#### use_idf: True/False\n",
    "#### tokenizer: None, tokenizer.tokenize\n",
    "#### criterion: gini, entropy\n",
    "#### stopwords\n",
    "\n",
    "params = {'text_pipe__tf_transformer__use_idf':[True, False],\n",
    "         'text_pipe__count_vectorizer__tokenizer':[None,tokenizer.tokenize],\n",
    "         'text_pipe__count_vectorizer__stop_words':[None,stopwords_list],\n",
    "         'clf__criterion':['gini', 'entropy']}\n",
    "\n",
    "## Make and fit grid\n",
    "grid = GridSearchCV(full_pipe,params,cv=3)\n",
    "grid.fit(X_train,y_train)\n",
    "## Display best params\n",
    "grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.422610Z",
     "start_time": "2020-12-01T18:41:47.404729Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evluate the best_estimator\n",
    "best_pipe = grid.best_estimator_\n",
    "y_hat_test = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.595725Z",
     "start_time": "2020-12-01T18:41:47.423995Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(y_test,y_hat_test,X_test,best_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importances as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.619512Z",
     "start_time": "2020-12-01T18:41:47.597400Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.625041Z",
     "start_time": "2020-12-01T18:41:47.621427Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.631932Z",
     "start_time": "2020-12-01T18:41:47.626957Z"
    }
   },
   "outputs": [],
   "source": [
    "features = text_pipe.named_steps['count_vectorizer'].get_feature_names()\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:47.646787Z",
     "start_time": "2020-12-01T18:41:47.643376Z"
    }
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.117075Z",
     "start_time": "2020-12-01T18:41:47.649567Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "rf = best_pipe.named_steps['clf']\n",
    "with plt.style.context('seaborn-talk'):\n",
    "    importance = pd.Series(rf.feature_importances_,index= features)#vectorizer.get_feature_names())\n",
    "    importance.sort_values(inplace=True)\n",
    "\n",
    "    importance.sort_values().tail(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.118590Z",
     "start_time": "2020-12-01T18:41:23.112Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[df['text'].str.contains('...',regex=False)]['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.119866Z",
     "start_time": "2020-12-01T18:41:23.115Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_word_probs = {}\n",
    "for word in importance.tail(20).index:\n",
    "    rows = df['text'].str.contains(word,regex=False,case=False)\n",
    "    val_count= df[rows]['source'].value_counts(normalize=True)\n",
    "    top_word_probs[word] = val_count\n",
    "#     print(f'\\n\\n{word}:\\n{val_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.120950Z",
     "start_time": "2020-12-01T18:41:23.118Z"
    }
   },
   "outputs": [],
   "source": [
    "top_probs = pd.DataFrame(top_word_probs).T\n",
    "top_probs.style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE (for Student Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.121931Z",
     "start_time": "2020-12-01T18:41:23.122Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.123192Z",
     "start_time": "2020-12-01T18:41:23.125Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.124227Z",
     "start_time": "2020-12-01T18:41:23.128Z"
    }
   },
   "outputs": [],
   "source": [
    "## TSNE For Visualizing High Dimensional Data\n",
    "t_sne_object_3d = TSNE(n_components=3)\n",
    "transformed_data_3d = t_sne_object_3d.fit_transform(X_train_pipe)\n",
    "transformed_data_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.125363Z",
     "start_time": "2020-12-01T18:41:23.131Z"
    }
   },
   "outputs": [],
   "source": [
    "## Separate into Trump/Not Trump\n",
    "trump = transformed_data_3d[y_train==1]\n",
    "not_trump = transformed_data_3d[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.126442Z",
     "start_time": "2020-12-01T18:41:23.135Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(trump[:,0],trump[:,1],\n",
    "           trump[:,2],c='orange',label='Trump')\n",
    "ax.scatter(not_trump[:,0],not_trump[:,1],\n",
    "           not_trump[:,2],c='black',label='Not Trump')\n",
    "ax.legend()\n",
    "ax.view_init(30, 10)\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.127412Z",
     "start_time": "2020-12-01T18:41:23.138Z"
    }
   },
   "outputs": [],
   "source": [
    "## TSNE For Visualizing High Dimensional Data\n",
    "t_sne_object_2d = TSNE(n_components=2)\n",
    "transformed_data_2d = t_sne_object_2d.fit_transform(X_train_pipe)\n",
    "## Separate into Trump/Not Trump\n",
    "trump = transformed_data_2d[y_train==1]\n",
    "not_trump = transformed_data_2d[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.128419Z",
     "start_time": "2020-12-01T18:41:23.141Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "ax.scatter(trump[:,0],trump[:,1],c='orange',label='Trump')\n",
    "ax.scatter(not_trump[:,0],not_trump[:,1],c='black',label='Not Trump')\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Classifiers - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T18:41:48.129475Z",
     "start_time": "2020-12-01T18:41:23.145Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()#alpha = 1.0e-08)\n",
    "nb_classifier.fit(X_train_pipe,y_train)\n",
    "y_hat_test = nb_classifier.predict(X_test_pipe)\n",
    "evaluate_model(y_test,y_hat_test,X_test_pipe,nb_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
