{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 39: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xx/xx/xx\n",
    "- my-best-lesson-plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduce the field of Natural Language Processing\n",
    "- Learn about the extensive preprocessing involved with text data\n",
    "- Walk through text classification - Finding Trump \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions from Gdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Questions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is NLP Used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Reviews (i.e. Amazon)\n",
    "- Stock market trading\n",
    "- AI Assistants\n",
    "- Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Demonstration:**\n",
    "    - [Google Duplex AI Assistant](https://youtu.be/D5VN56jQMWM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing text data requires more processing than normal data.**\n",
    "1. We must remove things like:\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters\n",
    "    \n",
    "    \n",
    "2. It is always recommended that go a step beyond this and remove **commonly used words that contain little information (called \"stopwords\")** for our machine learning algorithms. Words like: the,was,he,she, it,etc.\n",
    "\n",
    "\n",
    "3. Additionally, most analyses **need the text tokenzied** into a list of words and not in a natural sentence format. Instead, they are a list of words (**tokens**) separated by \"`,`\", which tells the algorithm what should be considered one word.\n",
    "\n",
    "\n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "There are often **multiple variants of the same word with the same/simiar meaning**,<br> but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).**<br> Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><br> A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"<br>  \n",
    "    - Stemming<br><img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-nlp-and-word-vectorization-online-ds-ft-100719/master/images/new_stemming.png\" width=40%>\n",
    "    - Lemmatization\n",
    "    \n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |\n",
    "\n",
    "5. Finally, we have to convert our text data into numeric form for our machine learning models to analyze, a process called **vectorization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For computers to process text it needs to be converted to a numerical representation of the text.\n",
    "- **There are several different ways we can vectorize our text:**\n",
    "    - Count vectorization\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "        -  Used for multiple texts\n",
    "    - Word Embeddings (Deep NLP)\n",
    "    \n",
    "    \n",
    ">- **_Term Frequency_** is calculated with the following formula:\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ <br>\n",
    "- Which can also be represented as:\n",
    "$$\\begin{align}\n",
    " \\text{tf}_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $$\n",
    "\n",
    "> - **_Inverse Document Frequency_** is calculated with the following formula:\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$<br>\n",
    "- Which can also be represented as: \n",
    "$$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $$\n",
    "\n",
    "> The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n",
    "$$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing } i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $$\n",
    "\n",
    "- There are additional ways to vectorize using Deep Neural Networks to create Word Embeddings (see Module 4 > Appendix: Deep NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Do we remove stop words or not?    \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?   \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?   \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Text Preprocessing - Finding Trump "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Natural Language Processing Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare Donal Trump's tweets for modeling, **it is essential to preprocess the text** and simplify its contents.\n",
    "<br><br>\n",
    "1. **At a minimum, things like:**\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters<br>\n",
    "    ***must*** be addressed before any initial analyses. I refer tho this initial cleaning as **\"minimal cleaning\"** of the text content<br>\n",
    "    \n",
    "> Version 1 of the tweet processing removes these items, as well as the removal of any urls in a tweet. The resulting data column is referred to here as \"content_min_clean\".\n",
    "\n",
    "<br><br>\n",
    "2. It is **always recommended** that go a step beyond this and<br> remove **commonly used words that contain little information** <br>for our machine learning algorithms. Words like: (the,was,he,she, it,etc.)<br> are called **\"stopwords\"**, and it is critical to address them as well.\n",
    "\n",
    "> Version 2 of the tweet processing removes these items and the resulting data column is referred here as `cleaned_stopped_content`\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Additionally, many analyses **need the text tokenzied** into a list of words<br> and not in a natural sentence format. Instead, they are a list of words (**tokens**) separated by \",\", which tells the algorithm what should be considered one word.<br><br>For the tweet processing, I used a version of tokenization, called `regexp_tokenziation` <br>which uses pattern of letters and symbols (the `expression`) <br>that indicate what combination of alpha numeric characters should be considered a single token.<br><br>The pattern I used was `\"([a-zA-Z]+(?:'[a-z]+)?)\"`, which allows for words such as \"can't\" that contain \"'\" in the middle of word. This processes was actually applied in order to process Version 1 and 2 of the Tweets, but the resulting text was put back into sentence form. \n",
    "\n",
    "> Version 3 of the tweets keeps the text in their regexp-tokenized form and is reffered to as `cleaned_stopped_tokens`\n",
    "<br>\n",
    "\n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "There are often **multiple variants of the same word with the same/simiar meaning**,<br> but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).**<br> Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><br> A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"<br>  \n",
    "\n",
    "> Version 4 of the tweets are all reduced down to their word lemmas, futher aiding the algorithm in learning the meaning of the texts.\n",
    "<!-- \n",
    "\n",
    "#### EXAMPLE TWEETS AND PROCESSING STEPS:\n",
    "\n",
    "**TWEET FROM 08-25-2017 12:25:10:**\n",
    "* **[\"content\"] column:**<p><blockquote>***\"Strange statement by Bob Corker considering that he is constantly asking me whether or not he should run again in '18. Tennessee not happy!\"***\n",
    "    \n",
    "    \n",
    "* **[\"content_min_clean\"] column:**<p><blockquote>***\"strange statement by bob corker considering that he is constantly asking me whether or not he should run again in  18  tennessee not happy \"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_content\"] column:**<p><blockquote>***\"strange statement bob corker considering constantly asking whether run tennessee happy\"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_tokens\"] column:**<p><blockquote>***\"['strange', 'statement', 'bob', 'corker', 'considering', 'constantly', 'asking', 'whether', 'run', 'tennessee', 'happy']\"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_lemmas\"] column:**<p><blockquote>***\"strange statement bob corker considering constantly asking whether run tennessee happy\"*** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Trump - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:46:01.511855Z",
     "start_time": "2020-10-19T15:45:58.394379Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:47:56.962664Z",
     "start_time": "2020-10-19T15:47:56.903487Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "finding_trump = 'finding-trump.csv'\n",
    "df = pd.read_csv(finding_trump)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:25.506821Z",
     "start_time": "2020-10-19T15:16:25.502385Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create a variable \"corpus\" containing all text\n",
    "corpus = df['text'].to_list()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Bag-of-Words Frequency Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"bag-of-words\": collection of all words from a corpus and their frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:25.512296Z",
     "start_time": "2020-10-19T15:16:25.508960Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:48:13.259102Z",
     "start_time": "2020-10-19T15:48:13.257202Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a FreqDist from the corpus\n",
    "\n",
    "## Display 100 most common words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> That's not quite right..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:48:25.576447Z",
     "start_time": "2020-10-19T15:48:25.227747Z"
    }
   },
   "outputs": [],
   "source": [
    "## Tokenize corpus then generate FreqDist\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Better...but what's our next issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:29.923785Z",
     "start_time": "2020-10-19T15:16:29.921705Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:49:17.686758Z",
     "start_time": "2020-10-19T15:49:17.684587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "\n",
    "## Add punctuation to stopwords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:29.929446Z",
     "start_time": "2020-10-19T15:16:29.927191Z"
    }
   },
   "outputs": [],
   "source": [
    "## Some additional Tweet Punctuation\n",
    "additional_punc = ['“','”','...',\"''\",'’','``']\n",
    "stopwords_list.extend(additional_punc)#['“','”'])#'...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:49:31.795795Z",
     "start_time": "2020-10-19T15:49:31.376037Z"
    }
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "print('until' in stopwords_list)\n",
    "\n",
    "stopwords_list.remove('until')\n",
    "print('until' in stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:31.123754Z",
     "start_time": "2020-10-19T15:16:29.943307Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove stopwords and then re-product the FreqDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ways to Show Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Word Clouds](https://www.geeksforgeeks.org/generating-word-cloud-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:32.979255Z",
     "start_time": "2020-10-19T15:16:32.326746Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(stopwords=stopwords_list,collocations=False)\n",
    "wordcloud.generate(','.join(stopped_tokens))\n",
    "plt.figure(figsize = (12, 12), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:33.001826Z",
     "start_time": "2020-10-19T15:16:32.980540Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What recognizable pattern of characters is high on the frequency list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Bag of Words Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.193708Z",
     "start_time": "2020-10-19T15:16:33.003886Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweet_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.193708Z",
     "start_time": "2020-10-19T15:16:33.003886Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a DataFrame from the Bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.695409Z",
     "start_time": "2020-10-19T15:16:34.195379Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "tweet_pmi_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweet_pmi_finder.apply_freq_filter(5)\n",
    "\n",
    "tweet_pmi_scored = tweet_pmi_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.695409Z",
     "start_time": "2020-10-19T15:16:34.195379Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a DataFrame from the Bigrams with PMI\n",
    "pd.DataFrame.from_records(tweet_pmi_scored,columns=['Words','PMI']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regular expressions can help us capture/remove complicated patterns in our text.\n",
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side.\n",
    "    \n",
    "    \n",
    "- Let's use regular expressions to remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:51:56.577398Z",
     "start_time": "2020-10-19T15:51:56.478656Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-899710885ed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Select an example tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6615\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "## Select an example tweet\n",
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:51:56.899487Z",
     "start_time": "2020-10-19T15:51:56.893303Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e25bca6f0e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Select a second example tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7347\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "## Select a second example tweet\n",
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.710477Z",
     "start_time": "2020-10-19T15:16:34.706835Z"
    }
   },
   "outputs": [],
   "source": [
    "## From tjhe lessons\n",
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use regex to find/remove URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- www.regex101.com\n",
    "    - Copy and paste example text to search\n",
    "    - Test out regular expressions and see what they pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.714829Z",
     "start_time": "2020-10-19T15:16:34.712367Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.719408Z",
     "start_time": "2020-10-19T15:16:34.716429Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r\"(https://\\w*\\.\\w*/+\\w+)\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.728427Z",
     "start_time": "2020-10-19T15:16:34.724431Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    \n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.734437Z",
     "start_time": "2020-10-19T15:16:34.730774Z"
    }
   },
   "outputs": [],
   "source": [
    "## Other uses of RegEx for Tweet preprocessing\n",
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.739637Z",
     "start_time": "2020-10-19T15:16:34.736222Z"
    }
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.744593Z",
     "start_time": "2020-10-19T15:16:34.741268Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.749590Z",
     "start_time": "2020-10-19T15:16:34.746240Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.754736Z",
     "start_time": "2020-10-19T15:16:34.751530Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "text_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.761577Z",
     "start_time": "2020-10-19T15:16:34.756580Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.787884Z",
     "start_time": "2020-10-19T15:16:34.763380Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:34.793442Z",
     "start_time": "2020-10-19T15:16:34.789649Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVITY: FINDING TRUMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Trump with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:35.979043Z",
     "start_time": "2020-10-19T15:16:34.795447Z"
    }
   },
   "outputs": [],
   "source": [
    "finding_trump = '../finding-trump.csv'#'https://raw.githubusercontent.com/jirvingphd/online-ds-pt-1007109-text-classification-finding-trump/master/finding-trump.csv'\n",
    "\n",
    "df = pd.read_csv(finding_trump,#'https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv',\n",
    "                index_col='created_at',parse_dates=['created_at'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early During His Presidency, Trump used an unofficial Android Phone  \n",
    "> During this time period, his staffers were the ones Tweeting from the official iPhone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:35.985929Z",
     "start_time": "2020-10-19T15:16:35.980853Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check Value Counts for Source\n",
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:36.004980Z",
     "start_time": "2020-10-19T15:16:35.987412Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get time period where Trump still had his personal Android\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:36.011730Z",
     "start_time": "2020-10-19T15:16:36.006361Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check new value counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:36.025175Z",
     "start_time": "2020-10-19T15:16:36.013516Z"
    }
   },
   "outputs": [],
   "source": [
    "## What do the \"Web\" tweets look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:36.043013Z",
     "start_time": "2020-10-19T15:16:36.026699Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove the Web tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:16:36.050081Z",
     "start_time": "2020-10-19T15:16:36.044797Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T14:53:40.873199Z",
     "start_time": "2020-10-19T14:53:40.865720Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make new Trump Tweet Column of 0 and 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T14:53:40.880379Z",
     "start_time": "2020-10-19T14:53:40.875040Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T14:53:40.894149Z",
     "start_time": "2020-10-19T14:53:40.888526Z"
    }
   },
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T14:53:40.901767Z",
     "start_time": "2020-10-19T14:53:40.895870Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check y_train and y_test value counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T14:53:40.919840Z",
     "start_time": "2020-10-19T14:53:40.917523Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:27:21.495394Z",
     "start_time": "2020-10-19T15:27:21.413309Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "## Make TfidfVectorizer\n",
    "\n",
    "\n",
    "# Make X_train_tfidf and X_test_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:27:34.567281Z",
     "start_time": "2020-10-19T15:27:34.350383Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make and fit a random forest\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "rf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:27:35.937089Z",
     "start_time": "2020-10-19T15:27:35.897710Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get predictions\n",
    "y_hat_test = rf.predict(X_test_tfidf)\n",
    "y_hat_train = rf.predict(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:33:01.112623Z",
     "start_time": "2020-10-19T15:33:00.916952Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# my_scorer = metrics.make_scorer(evaluate_model,)\n",
    "\n",
    "def evaluate_model(y_test,y_hat_test,X_test,clf=None,\n",
    "                  scoring=metrics.recall_score,verbose=False,\n",
    "                  scorer=False,classes=['Not Trump','Trump']):\n",
    "\n",
    "    print(metrics.classification_report(y_test,y_hat_test,\n",
    "                                        target_names=classes))\n",
    "    \n",
    "    metrics.plot_confusion_matrix(clf,X_test,y_test,normalize='true',\n",
    "                                 cmap='Blues',display_labels=classes)\n",
    "    plt.show()\n",
    "    if verbose:\n",
    "        print(\"MODEL PARAMETERS:\")\n",
    "        print(pd.Series(rf.get_params()))\n",
    "        \n",
    "    if scorer:\n",
    "        \n",
    "        return scoring(y_test,y_hat_test)\n",
    "    \n",
    "    \n",
    "## Evaluate MOdel\n",
    "evaluate_model(y_test,y_hat_test,X_test_tfidf,rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note About Pipelines and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may want to to this process in multiple steps (first Count Vectorize, then transform to TF or TF-IDF.\n",
    "- Can then use these in a Pipeline to be able to GridSearch more aspect of the text preprocessing\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "#tf_transformer.fit(X_train_counts)\n",
    "#X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "#X_train_tf.shape\n",
    "\n",
    "\n",
    "text_pipe = Pip\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:33:14.344783Z",
     "start_time": "2020-10-19T15:33:14.340419Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tf_transform = TfidfTransformer(use_idf=True)\n",
    "\n",
    "text_pipe = Pipeline(steps=[\n",
    "    ('count_vectorizer',count_vect),\n",
    "    ('tf_transformer',tf_transform)])\n",
    "\n",
    "full_pipe = Pipeline(steps=[\n",
    "    ('text_pipe',text_pipe),\n",
    "    ('clf',RandomForestClassifier(class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:33:14.753049Z",
     "start_time": "2020-10-19T15:33:14.748158Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:33:15.314551Z",
     "start_time": "2020-10-19T15:33:15.292123Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:33:16.371436Z",
     "start_time": "2020-10-19T15:33:16.016726Z"
    }
   },
   "outputs": [],
   "source": [
    "## Modeling with full pipeline\n",
    "full_pipe.fit(X_train,y_train)\n",
    "y_hat_test = full_pipe.predict(X_test)\n",
    "evaluate_model(y_test,y_hat_test,X_test,full_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:35:16.923979Z",
     "start_time": "2020-10-19T15:35:16.921485Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='text')\n",
    "\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:40:15.445354Z",
     "start_time": "2020-10-19T15:40:08.010294Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False,)\n",
    "\n",
    "params = {'text_pipe__tf_transformer__use_idf':[True,False],\n",
    "         'text_pipe__count_vectorizer__tokenizer':[None,tokenizer.tokenize],\n",
    "         'clf__criterion':['gini','entropy']}\n",
    "\n",
    "grid = GridSearchCV(full_pipe,params)\n",
    "grid.fit(X_train,y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:41:07.355994Z",
     "start_time": "2020-10-19T15:41:07.163877Z"
    }
   },
   "outputs": [],
   "source": [
    "best_pipe = grid.best_estimator_\n",
    "y_hat_test = best_pipe.predict(X_test)\n",
    "evaluate_model(y_test,y_hat_test,X_test,clf=best_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importances as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:41:43.133237Z",
     "start_time": "2020-10-19T15:41:43.110661Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:41:50.588675Z",
     "start_time": "2020-10-19T15:41:50.584466Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:42:27.358184Z",
     "start_time": "2020-10-19T15:42:27.353233Z"
    }
   },
   "outputs": [],
   "source": [
    "features = text_pipe.named_steps['count_vectorizer'].get_feature_names()\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:42:38.456494Z",
     "start_time": "2020-10-19T15:42:38.453015Z"
    }
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:42:43.507793Z",
     "start_time": "2020-10-19T15:42:43.213226Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "with plt.style.context('seaborn-talk'):\n",
    "    importance = pd.Series(rf.feature_importances_,index= vectorizer.get_feature_names())\n",
    "    importance.sort_values(inplace=True)\n",
    "\n",
    "    importance.sort_values().tail(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:56:12.596704Z",
     "start_time": "2020-10-19T15:56:12.594555Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[df['text'].str.contains('...',regex=False)]['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:42:50.159776Z",
     "start_time": "2020-10-19T15:42:50.115372Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_word_probs = {}\n",
    "for word in importance.tail(20).index:\n",
    "    rows = df['text'].str.contains(word,regex=False,case=False)\n",
    "    val_count= df[rows]['source'].value_counts(normalize=True)\n",
    "    top_word_probs[word] = val_count\n",
    "#     print(f'\\n\\n{word}:\\n{val_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:42:54.429813Z",
     "start_time": "2020-10-19T15:42:54.399705Z"
    }
   },
   "outputs": [],
   "source": [
    "top_probs = pd.DataFrame(top_word_probs).T\n",
    "top_probs.style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE (for Student Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:05.529988Z",
     "start_time": "2020-10-19T15:43:05.523007Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:00:52.235800Z",
     "start_time": "2020-10-19T15:00:52.225490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:16.366269Z",
     "start_time": "2020-10-19T15:43:14.432839Z"
    }
   },
   "outputs": [],
   "source": [
    "## TSNE For Visualizing High Dimensional Data\n",
    "t_sne_object_3d = TSNE(n_components=3)\n",
    "transformed_data_3d = t_sne_object_3d.fit_transform(X_train_pipe)\n",
    "transformed_data_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:17.334538Z",
     "start_time": "2020-10-19T15:43:17.331423Z"
    }
   },
   "outputs": [],
   "source": [
    "## Separate into Trump/Not Trump\n",
    "trump = transformed_data_3d[y_train==1]\n",
    "not_trump = transformed_data_3d[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:17.901760Z",
     "start_time": "2020-10-19T15:43:17.671394Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(trump[:,0],trump[:,1],\n",
    "           trump[:,2],c='orange',label='Trump')\n",
    "ax.scatter(not_trump[:,0],not_trump[:,1],\n",
    "           not_trump[:,2],c='black',label='Not Trump')\n",
    "ax.legend()\n",
    "ax.view_init(30, 10)\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:23.457339Z",
     "start_time": "2020-10-19T15:43:21.993758Z"
    }
   },
   "outputs": [],
   "source": [
    "## TSNE For Visualizing High Dimensional Data\n",
    "t_sne_object_2d = TSNE(n_components=2)\n",
    "transformed_data_2d = t_sne_object_2d.fit_transform(X_train_pipe)\n",
    "## Separate into Trump/Not Trump\n",
    "trump = transformed_data_2d[y_train==1]\n",
    "not_trump = transformed_data_2d[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:26.303799Z",
     "start_time": "2020-10-19T15:43:26.061622Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "ax.scatter(trump[:,0],trump[:,1],c='orange',label='Trump')\n",
    "ax.scatter(not_trump[:,0],not_trump[:,1],c='black',label='Not Trump')\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Classifiers - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:47.122621Z",
     "start_time": "2020-10-19T15:43:46.959235Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()#alpha = 1.0e-08)\n",
    "nb_classifier.fit(X_train_pipe,y_train)\n",
    "y_hat_test = nb_classifier.predict(X_test_pipe)\n",
    "evaluate_model(y_test,y_hat_test,X_test_pipe,nb_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:43:53.286265Z",
     "start_time": "2020-10-19T15:43:53.284120Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# params  = {'criterion':['gini','entropy'],\n",
    "#            'max_depth':[3,5,10,50,100,None],\n",
    "#           'class_weight':['balanced',None],\n",
    "#            'bootstrap':[True ,False],\n",
    "#           'min_samples_leaf':[1,2,3,4],\n",
    "#           }\n",
    "# rf_clf = RandomForestClassifier()\n",
    "# grid = GridSearchCV(rf_clf,params,return_train_score=False,\n",
    "#                     scoring='recall_weighted',n_jobs=-1)\n",
    "# grid.fit(X_train_tfidf,y_train)\n",
    "# print(grid.best_score_)\n",
    "# grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:44:00.271561Z",
     "start_time": "2020-10-19T15:44:00.269415Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_rf = grid.best_estimator_\n",
    "# best_rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# y_hat_test = best_rf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:44:01.950400Z",
     "start_time": "2020-10-19T15:44:01.948246Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_model(y_test,y_hat_test,X_test_tfidf,best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:44:04.347386Z",
     "start_time": "2020-10-19T15:44:04.345387Z"
    }
   },
   "outputs": [],
   "source": [
    "# importance = pd.Series(best_rf.feature_importances_,index= vectorizer.get_feature_names())\n",
    "# importance.sort_values().tail(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOOKMARK: Better Handling Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://medium.com/towards-artificial-intelligence/emoticon-and-emoji-in-text-mining-7392c49f596a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluded Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary table from Finding Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T22:52:47.456390Z",
     "start_time": "2020-10-17T22:52:47.417051Z"
    }
   },
   "outputs": [],
   "source": [
    "## Summary Table with Most Frequent Words \n",
    "prob_cols =['Twitter for Android','Twitter for iPhone']\n",
    "top_probs['importance'] = importance.loc[top_probs.index]\n",
    "\n",
    "top_probs['max_prob'] = top_probs[prob_cols].max(axis=1)\n",
    "top_probs['Max Prob Class'] = top_probs[prob_cols].idxmax(axis=1)\n",
    "top_probs.sort_values('importance',0,0,inplace=True)\n",
    "top_probs.style.bar('importance')\\\n",
    "                    .background_gradient(subset=['max_prob'])\\\n",
    "                    .highlight_max(subset=prob_cols,axis=1,color='lightgreen')\n",
    "#.background_gradient(subset=prob_cols,axis=1,cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T22:52:47.481561Z",
     "start_time": "2020-10-17T22:52:47.458342Z"
    }
   },
   "outputs": [],
   "source": [
    "results = top_probs[['Max Prob Class','max_prob','importance']]\n",
    "display(results.style.bar('importance').background_gradient(subset=['max_prob']))\n",
    "results['Max Prob Class'].value_counts(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Tokenizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T22:52:47.509434Z",
     "start_time": "2020-10-17T22:52:47.483975Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Vocabulary\n",
    "- Corpus\n",
    "    - Body of text\n",
    "    \n",
    "- Bag of Words\n",
    "    - Collection of all words from a corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use https://regex101.com/ to test out regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Free Grammers and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-context-free-grammars-and-POS-tagging-online-ds-ft-100719/master/images/new_LevelsOfLanguage-Graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax and Meaning Can be Difficult for Computers \n",
    "\n",
    "In English, sentences consist of a **_Noun Phrase_** followed by a **_Verb Phrase_**, which may optionally be followed by a **_Prepositional Phrase_**.\n",
    "\n",
    "This ***seems simple, but it gets more tricky*** when we realize that there is a recursive structure to these phrases.\n",
    "\n",
    "- A noun phrase may consist of multiple smaller noun phrases, and in some cases, even a verb phrase. \n",
    "- Similarly, a verb phrase can consist of multiple smaller verb phrases and noun phrases, which can themselves be made up of smaller noun phrases and verb phrases. \n",
    "\n",
    "\n",
    "This leads levels of **_ambiguity_** that can be troublesome for computers. NLTK's documentation explains this by examining the classic Groucho Marx joke:\n",
    "\n",
    "> ***\"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-context-free-grammars-and-POS-tagging-online-ds-ft-100719/master/images/parse_tree.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
