{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 43: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss word Embeddings and their advantages\n",
    "- Training Word2Vec models\n",
    "- Using pretrained word embeddings\n",
    "\n",
    "\n",
    "- Create a Classification Model for true-trump (\"Twitter for Android\") vs trump-staffer(\"Twitter for iPhone - from period of time when android was still in use)\n",
    "\n",
    "    - Use lesson's W2Vec class in Sci-kit learn models\n",
    "    - Use LSTMs\n",
    "    - Use RNN/GRUs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Compare:\n",
    "    1.  Mean embeddings vs count/tfidf data with scikit learn.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Do we remove stop words or not?    \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?   \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?   \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:18.137657Z",
     "start_time": "2020-02-17T03:00:13.194663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds_1007219  v0.7.6 loaded.  Read the docs: https://fsds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_a0684162_5131_11ea_a23b_acde48001122\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row1_col1\" class=\"data row1 col1\" >fsds_100719</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_a0684162_5131_11ea_a23b_acde48001122row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a1c91ac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[i] Pandas .iplot() method activated.']\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:20.092776Z",
     "start_time": "2020-02-17T03:00:18.139116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:37:57</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:38:09</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 22:52:10</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-02 02:45:18</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-03 00:44:20</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:17:43</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:18:47</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:22:28</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:30:35</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 03:12:07</td>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source  \\\n",
       "datetime                                    \n",
       "2016-12-01 14:37:57    Twitter for iPhone   \n",
       "2016-12-01 14:38:09   Twitter for Android   \n",
       "2016-12-01 22:52:10    Twitter for iPhone   \n",
       "2016-12-02 02:45:18    Twitter for iPhone   \n",
       "2016-12-03 00:44:20   Twitter for Android   \n",
       "...                                   ...   \n",
       "2020-01-01 01:17:43    Twitter for iPhone   \n",
       "2020-01-01 01:18:47    Twitter for iPhone   \n",
       "2020-01-01 01:22:28    Twitter for iPhone   \n",
       "2020-01-01 01:30:35    Twitter for iPhone   \n",
       "2020-01-01 03:12:07  Twitter Media Studio   \n",
       "\n",
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "...                                                                ...   \n",
       "2020-01-01 01:17:43  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "2020-01-01 01:18:47            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "2020-01-01 01:22:28  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "2020-01-01 01:30:35                                    HAPPY NEW YEAR!   \n",
       "2020-01-01 03:12:07                            https://t.co/EVAEYD1AgV   \n",
       "\n",
       "                              created_at  retweet_count  favorite_count  \\\n",
       "datetime                                                                  \n",
       "2016-12-01 14:37:57  12-01-2016 14:37:57          12077           65724   \n",
       "2016-12-01 14:38:09  12-01-2016 14:38:09           9834           57249   \n",
       "2016-12-01 22:52:10  12-01-2016 22:52:10           5564           31256   \n",
       "2016-12-02 02:45:18  12-02-2016 02:45:18          17283           72196   \n",
       "2016-12-03 00:44:20  12-03-2016 00:44:20          24700          111106   \n",
       "...                                  ...            ...             ...   \n",
       "2020-01-01 01:17:43  01-01-2020 01:17:43           8893               0   \n",
       "2020-01-01 01:18:47  01-01-2020 01:18:47          10796               0   \n",
       "2020-01-01 01:22:28  01-01-2020 01:22:28          27567          132633   \n",
       "2020-01-01 01:30:35  01-01-2020 01:30:35          85409          576045   \n",
       "2020-01-01 03:12:07  01-01-2020 03:12:07          25016          108830   \n",
       "\n",
       "                    is_retweet               id_str  \n",
       "datetime                                             \n",
       "2016-12-01 14:37:57      False   804333718999539712  \n",
       "2016-12-01 14:38:09      False   804333771021570048  \n",
       "2016-12-01 22:52:10      False   804458095569158144  \n",
       "2016-12-02 02:45:18      False   804516764562374656  \n",
       "2016-12-03 00:44:20      False   804848711599882240  \n",
       "...                        ...                  ...  \n",
       "2020-01-01 01:17:43       True  1212181071988703232  \n",
       "2020-01-01 01:18:47       True  1212181341078458369  \n",
       "2020-01-01 01:22:28      False  1212182267113680896  \n",
       "2020-01-01 01:30:35      False  1212184310389850119  \n",
       "2020-01-01 03:12:07      False  1212209862094012416  \n",
       "\n",
       "[14066 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T23:02:01.540856Z",
     "start_time": "2020-02-16T23:02:01.536707Z"
    }
   },
   "source": [
    "### Making iPhone vs Twitter df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:20.096806Z",
     "start_time": "2020-02-17T03:00:20.094659Z"
    }
   },
   "outputs": [],
   "source": [
    "devices = ['Twitter for Android','Twitter for iPhone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:20.106450Z",
     "start_time": "2020-02-17T03:00:20.098227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first and last timestamps for the group Twitter for Androidare:\n",
      "2016-12-01 14:38:09\n",
      "2017-03-25 14:41:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The first and last timestamps for the group {devices[0]}are:')\n",
    "start,end= df.groupby('source').get_group(devices[0]).index[[0,-1]]\n",
    "print(start) ,print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:20.119896Z",
     "start_time": "2020-02-17T03:00:20.107913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter for Android    0.603648\n",
       "Twitter for iPhone     0.396352\n",
       "Name: source, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Puttig it all toether\n",
    "df_data = df[df['source'].isin(devices)].loc[start:end]\n",
    "df_data['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:20.306826Z",
     "start_time": "2020-02-17T03:00:20.121620Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('datasets/trump_tweets_iphone_vs_twitter.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T02:26:31.083813Z",
     "start_time": "2020-02-17T02:26:31.080306Z"
    }
   },
   "source": [
    "- Train word embeddings on miminally processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:24.506115Z",
     "start_time": "2020-02-17T03:00:20.309104Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "data = df['text'].map(word_tokenize)\n",
    "data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:24.517235Z",
     "start_time": "2020-02-17T03:00:24.507847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Heading',\n",
       "  'to',\n",
       "  'U.S.',\n",
       "  'Bank',\n",
       "  'Arena',\n",
       "  'in',\n",
       "  'Cincinnati',\n",
       "  'Ohio',\n",
       "  'for',\n",
       "  'a',\n",
       "  '7pm',\n",
       "  'rally',\n",
       "  '.',\n",
       "  'Join',\n",
       "  'me',\n",
       "  '!',\n",
       "  'Tickets',\n",
       "  ':',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/HiWqZvHv6M'],\n",
       " ['heading',\n",
       "  'to',\n",
       "  'u.s.',\n",
       "  'bank',\n",
       "  'arena',\n",
       "  'in',\n",
       "  'cincinnati',\n",
       "  'ohio',\n",
       "  'for',\n",
       "  'a',\n",
       "  '7pm',\n",
       "  'rally',\n",
       "  '.',\n",
       "  'join',\n",
       "  'me',\n",
       "  '!',\n",
       "  'tickets',\n",
       "  ':',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/hiwqzvhv6m'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2],data_lower[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train Word2Vec on minimally processed text.\n",
    "    - Try only removing uppercase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:27.814406Z",
     "start_time": "2020-02-17T03:00:24.518709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3327046, 4413210)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "TEXT_SOURCE = data_lower\n",
    "VECTOR_SIZE = 100\n",
    "model = Word2Vec(TEXT_SOURCE,size=VECTOR_SIZE,window=4,min_count=1,workers=4)\n",
    "model.train(data_lower, total_examples=model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:27.837324Z",
     "start_time": "2020-02-17T03:00:27.815777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hillary', 0.8966301679611206),\n",
       " ('crooked', 0.8911771178245544),\n",
       " ('h', 0.8010057210922241),\n",
       " ('fbi', 0.7868572473526001),\n",
       " ('dnc', 0.7865959405899048),\n",
       " ('emails', 0.7657970190048218),\n",
       " ('cl…', 0.7605175375938416),\n",
       " ('strzok', 0.759728193283081),\n",
       " ('deleted', 0.7320792078971863),\n",
       " ('comey', 0.7287471294403076)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = model.wv\n",
    "\n",
    "wv.most_similar('clinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:00:27.843762Z",
     "start_time": "2020-02-17T03:00:27.838835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uncomfortable', 0.6394579410552979),\n",
       " ('usa', 0.6315405964851379),\n",
       " ('future', 0.6265136003494263),\n",
       " ('arrangements', 0.6233097314834595),\n",
       " ('//t.co/5volztaorm', 0.6066502332687378),\n",
       " ('morphing', 0.5912973880767822),\n",
       " ('american', 0.5843404531478882),\n",
       " ('america…', 0.5766141414642334),\n",
       " ('nation', 0.574530839920044),\n",
       " ('//t.co/5jxdojpzmn', 0.5607695579528809)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['america'])#,positive=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math with Trumps Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:01:08.003216Z",
     "start_time": "2020-02-17T03:01:07.997452Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODE\n",
    "# def get_vector(string):#,wv=wv):\n",
    "#     return wv.get_vector(string)\n",
    "\n",
    "\n",
    "# def get_similar(vector):#,wv=wv):\n",
    "#     return wv.similar_by_vector(vector)\n",
    "\n",
    "\n",
    "# def check_vocab(word):\n",
    "#     return word in wv.vocab\n",
    "\n",
    "def word_math(wv,pos_words=['hillary'],neg_words=['bill'],\n",
    "              verbose=True,return_vec=False):\n",
    "    if isinstance(pos_words,str):\n",
    "        pos_words=[pos_words]\n",
    "    if isinstance(neg_words,str):\n",
    "        neg_words=[neg_words]\n",
    "        \n",
    "\n",
    "\n",
    "    pos_eqn = '+'.join(pos_words)\n",
    "    neg_eqn = '-'.join(neg_words)\n",
    "\n",
    "    print('---'*15)    \n",
    "    print(f\"[i] Result for:\\t{pos_eqn}{' - '+neg_eqn if len(neg_eqn)>0 else ' '}\")\n",
    "    print('---'*15)\n",
    "\n",
    "    answer = wv.most_similar(positive=pos_words,negative=neg_words)\n",
    "    \n",
    "    if verbose:\n",
    "          [print(f\"- {ans[0]} ({round(ans[1],3)})\") for ans in answer]\n",
    "          print('---'*15,'\\n\\n')\n",
    "\n",
    "    if return_vec: \n",
    "          return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:01:13.856053Z",
     "start_time": "2020-02-17T03:01:13.837623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "[i] Result for:\tamerica+crime \n",
      "---------------------------------------------\n",
      "- borders (0.705)\n",
      "- morally (0.702)\n",
      "- choice (0.678)\n",
      "- r (0.636)\n",
      "- strong (0.63)\n",
      "- usa (0.615)\n",
      "- future (0.601)\n",
      "- tough (0.598)\n",
      "- california (0.596)\n",
      "- cuts (0.595)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tdemocrats+russia \n",
      "---------------------------------------------\n",
      "- dems (0.787)\n",
      "- russians (0.755)\n",
      "- corruption (0.705)\n",
      "- russian (0.702)\n",
      "- collusion (0.655)\n",
      "- dnc (0.655)\n",
      "- facts (0.651)\n",
      "- hoax (0.64)\n",
      "- they (0.636)\n",
      "- answers (0.629)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trepublican - honor\n",
      "---------------------------------------------\n",
      "- democrat (0.679)\n",
      "- thedemocrats (0.566)\n",
      "- dem (0.506)\n",
      "- republicans (0.5)\n",
      "- party (0.497)\n",
      "- dems (0.478)\n",
      "- votes (0.478)\n",
      "- liberal (0.475)\n",
      "- democratic (0.471)\n",
      "- //t.co/neavcugpzz (0.456)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tman+power \n",
      "---------------------------------------------\n",
      "- person (0.774)\n",
      "- leader (0.709)\n",
      "- guy (0.686)\n",
      "- wisdom (0.684)\n",
      "- empathy (0.684)\n",
      "- fool (0.679)\n",
      "- thief (0.679)\n",
      "- star (0.666)\n",
      "- reputation (0.665)\n",
      "- magnet (0.646)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trussia+honor \n",
      "---------------------------------------------\n",
      "- privilege (0.689)\n",
      "- conversation (0.679)\n",
      "- phone (0.674)\n",
      "- denuclearization (0.661)\n",
      "- ties (0.66)\n",
      "- call (0.66)\n",
      "- regard (0.654)\n",
      "- ukraine (0.652)\n",
      "- perfect (0.642)\n",
      "- campaign (0.64)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tchina - tariff\n",
      "---------------------------------------------\n",
      "- what (0.456)\n",
      "- everything (0.443)\n",
      "- cushy (0.434)\n",
      "- pachamber (0.431)\n",
      "- whistleblower.but (0.43)\n",
      "- manage… (0.422)\n",
      "- stars. (0.419)\n",
      "- legit (0.411)\n",
      "- clarity (0.403)\n",
      "- //t.co/1mehizxkfg (0.4)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(wv,*eqn)\n",
    "#     word_math(wv2,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layers\n",
    "You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\n",
    "\n",
    "* The embedding layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer \n",
    "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer  \n",
    "* The size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
    "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step) \n",
    "\n",
    "\n",
    "[Keras Documentation for Embedding Layers](https://keras.io/layers/embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Mean Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:27.501187Z",
     "start_time": "2020-02-17T00:15:27.494815Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "data = df['text'].map(word_tokenize)\n",
    "X = list(map(lambda x: [w.lower() for w in x],data))\n",
    "y = df['source']\n",
    "\n",
    "X_idx = list(range(X.shape[0]))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "\n",
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_dict = {'count':X_tf,\n",
    "         'tfidf':X_tfidf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:13:32.510160Z",
     "start_time": "2020-02-17T03:13:32.504613Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:13:34.680282Z",
     "start_time": "2020-02-17T03:13:34.525428Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:05:57.585577Z",
     "start_time": "2020-02-17T00:05:57.583332Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:25.173149Z",
     "start_time": "2020-02-17T00:15:25.169860Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer(use_idf=False)#TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)#TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:25.721283Z",
     "start_time": "2020-02-17T00:15:25.717083Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "y = le.fit_transform(df['source'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:26.299633Z",
     "start_time": "2020-02-17T00:15:26.281684Z"
    }
   },
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(df['text'])\n",
    "X_tf = tf_transformer.fit_transform(X)\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:26.842330Z",
     "start_time": "2020-02-17T00:15:26.838657Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tf.shape,X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:30.459911Z",
     "start_time": "2020-02-17T00:15:28.806843Z"
    }
   },
   "outputs": [],
   "source": [
    "res = [['Method','Model',\"Result\"]]\n",
    "\n",
    "for tf_type,X_data in X_dict.items():\n",
    "    X_train, X_test,y_train, y_test = train_test_split_idx(\n",
    "        X_data,y,train_idx,test_idx)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "    \n",
    "#     rf = RandomForestClassifier(n_estimators=100,verbose=True)\n",
    "        cv_res = cross_val_score(model, X_train,y_train, cv=5)\n",
    "        res.append([tf_type,name,cv_res.mean()])\n",
    "\n",
    "pd.DataFrame(res[1:],columns=res[0]).sort_values(\"Result\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:30.465560Z",
     "start_time": "2020-02-17T00:15:30.461508Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rf_params = dict(n_estimators=100, verbose=True)\n",
    "\n",
    "rf =Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest',RandomForestClassifier(**rf_params))])\n",
    "\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:30.469648Z",
     "start_time": "2020-02-17T00:15:30.467165Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:32.573766Z",
     "start_time": "2020-02-17T00:15:30.945923Z"
    }
   },
   "outputs": [],
   "source": [
    "# res = [['Model','Score']]\n",
    "res=[['Model','Scores']]\n",
    "for (name, model) in models:\n",
    "    print(name)\n",
    "    cv_res = cross_val_score(model, data_lower, df['source'], cv=5).mean()\n",
    "    res.append([name,cv_res])\n",
    "    \n",
    "pd.DataFrame(res[1:],columns=res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Trained vs Pre-Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:10:51.969655Z",
     "start_time": "2020-02-17T03:10:51.967000Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '/Users/jamesirving/Datasets/'#glove.twitter.27B/'\n",
    "glove_file = folder+'glove.6B/glove.6B.50d.txt'#'glove.twitter.27B.50d.txt'\n",
    "glove_twitter_file = folder+'glove.twitter.27B/glove.twitter.27B.100d.txt'#'glove.twitter.27B/glove.twitter.27B.50d.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only the vectors needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:02:36.300807Z",
     "start_time": "2020-02-17T03:02:36.251614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24051"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This line of code for getting all words bugs me\n",
    "total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T03:10:58.925658Z",
     "start_time": "2020-02-17T03:10:52.224486Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open(glove_twitter_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:05:57.425212Z",
     "start_time": "2020-02-17T00:05:57.419961Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:05:57.572935Z",
     "start_time": "2020-02-17T00:05:57.426528Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = df['text'].map(word_tokenize)\n",
    "# data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now With Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files import keras_gridsearch as kg\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:15:32.810013Z",
     "start_time": "2020-02-17T00:15:32.805971Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:17:10.125313Z",
     "start_time": "2020-02-17T00:17:10.118646Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:21:53.613999Z",
     "start_time": "2020-02-17T00:21:53.610467Z"
    }
   },
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['source'])\n",
    "y_t=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T01:49:10.410094Z",
     "start_time": "2020-02-17T01:49:10.406557Z"
    }
   },
   "outputs": [],
   "source": [
    "max(list(map(lambda x: len(x) ,sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:21:53.641341Z",
     "start_time": "2020-02-17T00:21:53.615476Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 20000\n",
    "tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(data_lower)#df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data_lower)#df['text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(sequences, maxlen=100)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:22:06.383304Z",
     "start_time": "2020-02-17T00:22:06.377987Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_t,y, test_size=.15)\n",
    "# print(X_train.shape),print(y_test.shape)\n",
    "X_train, X_test,y_train,y_test =train_test_split_idx(X_t,y_t,train_idx,test_idx)\n",
    "X_train.shape,y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:22:17.918061Z",
     "start_time": "2020-02-17T00:22:06.837011Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "model=Sequential()\n",
    "model.add(Embedding(MAX_WORDS,EMBEDDING_SIZE))\n",
    "model.add(LSTM(25,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:18:01.708621Z",
     "start_time": "2020-02-17T00:18:00.772373Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T23:21:46.045570Z",
     "start_time": "2020-02-16T23:21:46.043290Z"
    }
   },
   "outputs": [],
   "source": [
    "# metrics.plot_confusion_matrix(model_sk,X_test,y_test.values.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Text Preprocessing with Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LAST CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:47.534626Z",
     "start_time": "2020-02-16T20:55:47.532114Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:47.540833Z",
     "start_time": "2020-02-16T20:55:47.536822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list+=string.punctuation\n",
    "print(stopwords_list)\n",
    "stopwords_list.remove('until')\n",
    "stopwords_list.extend(['“','...','”'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:47.545971Z",
     "start_time": "2020-02-16T20:55:47.542533Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "'until' in stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:48.628860Z",
     "start_time": "2020-02-16T20:55:47.547559Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "freq = FreqDist(stopped_tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:48.665530Z",
     "start_time": "2020-02-16T20:55:48.630454Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:48.670802Z",
     "start_time": "2020-02-16T20:55:48.666982Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Get FreqDist for Cleaned Text Data\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T20:55:48.674365Z",
     "start_time": "2020-02-16T20:55:48.672222Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def clean_text(text,exclude_words=['until']):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "#     ## tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.382152Z",
     "start_time": "2020-02-05T16:45:27.350720Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:33:00.427588Z",
     "start_time": "2020-02-05T17:33:00.423967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:33:00.879037Z",
     "start_time": "2020-02-05T17:33:00.873783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:39:37.243670Z",
     "start_time": "2020-02-05T17:39:37.238793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:39:47.967157Z",
     "start_time": "2020-02-05T17:39:47.962699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('[i] Word Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print('\\n[i] Regexp Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:40:27.416410Z",
     "start_time": "2020-02-05T17:40:27.411167Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.419890Z",
     "start_time": "2020-02-05T16:45:27.417783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def regexp_tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "#     print(f\"- Tweet #{i}:\\n\")\n",
    "#     print(corpus[i],'\\n')\n",
    "#     from nltk import regexp_tokenize\n",
    "#     pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#     tokens= regexp_tokenize(corpus[i],pattern)\n",
    "\n",
    "#     # It is usually a good idea to lowercase all tokens during this step, as well\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     print(tokens,end='\\n\\n')\n",
    "#     return print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:41:40.216542Z",
     "start_time": "2020-02-05T17:41:40.211310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:41:40.414919Z",
     "start_time": "2020-02-05T17:41:40.410981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.430054Z",
     "start_time": "2020-02-05T16:45:27.423484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:43:48.150618Z",
     "start_time": "2020-02-05T17:43:46.167428Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:54:38.969730Z",
     "start_time": "2020-02-05T17:54:38.963457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "\n",
    "# # urls = find_urls(text)\n",
    "# def clean_text(text,regex=True):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "#     ## tokenize text\n",
    "#     if regex:\n",
    "#         pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#         tokens= regexp_tokenize(text,pattern)\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens\n",
    "\n",
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:54:39.472657Z",
     "start_time": "2020-02-05T17:54:39.440971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:54:40.266948Z",
     "start_time": "2020-02-05T17:54:40.262936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Potential Tasks: Classify Android vs iPhone tweets (from period where Android tweets still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:55:23.351071Z",
     "start_time": "2020-02-05T17:55:21.827219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df\n",
    "\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:55:35.507169Z",
     "start_time": "2020-02-05T17:55:31.878608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(process_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:12.423626Z",
     "start_time": "2020-02-05T17:56:12.408720Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "android = df.groupby('source').get_group('Twitter for Android')\n",
    "android.index\n",
    "\n",
    "iphone = df.groupby('source').get_group('Twitter for iPhone').loc[:android.index[-1]]\n",
    "iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:48.102617Z",
     "start_time": "2020-02-05T17:56:48.097448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(android), len(iphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:48.659622Z",
     "start_time": "2020-02-05T17:56:48.650431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_corpus = pd.concat([iphone,android],axis=0)\n",
    "df_corpus['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Count vectorization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    -  Used for multiple texts\n",
    "    \n",
    "    \n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Questions/Topics \n",
    "- Next time: vectorization\n",
    "- Vs Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:51.217196Z",
     "start_time": "2020-02-05T17:56:51.214208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:54.768705Z",
     "start_time": "2020-02-05T17:56:54.750115Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(df_corpus['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.727px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
